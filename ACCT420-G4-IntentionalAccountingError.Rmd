---
title: "<center> ACCT420-G4-IntentionalAccountingError </center>"
author: <center> Adrian Kevin Wijono - G1029244T </center>  <center> Jordan Steve - </center>  <center> Lam - </center>  <center> Ong Wei Bin - </center>  <center> Tung -  </center>  <center> <h4> Prepared for Prof. Wang Jiwei </h4> </center>  <center> <h4> Submitted on 8 April 2019 </h4> </center>   
output:
  html_document:
    theme: paper
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: true
    self_contained: no
---

<center> ![](http://diysolarpanelsv.com/images/smu-logo-clipart-33.png){width=1000px} </center>

# Introduction

In the accounting field, an error in a submitted firm's annual report can be classified under accounting error (wrong figure stated due to miscalculation on the firm's part). Unfortunately, this error can be both intentional or not. This project covers our chosen machine learning model to better predict intentional accounting errors, also known as Accounting Fraud. 

The outcome variable is a binary classification, with existing labels on the training data provided in the project brief. This classifies this project under a supervised learning project. 

Train and test data are provided, split by timestamp (train 2005-2009, test 2010) with the binary outcome variable omitted from the test data. As the data does not have any financial information, we extracted data from the CRSP/COMPUSTAT MERGED database from WRDS to build our model. These variables are either used as they are, or further processed into more useful measures such as ratios as inputs to the model.

An external research paper is also used as a reference for the variable selection. We ultimately chose 15 variables to be included in the models

Exploratory Data Analysis was also done to see the distribution of the output/outcome variable ("Restate_Int") by company, year, month and quarter to try and identify patterns to the occurence. It is apparent that the frequency of such accounting frauds are extremely rare and there isn't any noticeable pattern/trend. We also cleaned the data to control missing data and outlier effect as well as correlation factors. 

For the modelling, we utilized several different models and compared their out-of-sample AUC. We used Logistics Regression,Glmnet (LASSO, Ridge and Elastic Net), Gradient Boosting, Random Forest and a simple Neural Network. For some of the models, we proceeded to tune some parameters and cross validate them, before doing some post-processing logic to improve upon the initial model. 

We also tried to use an ensemble method utilizing the predictions from each of the aforementioned models. There will also be a discussion component where we discuss our chosen model and the practical significance of using them. 

# Variable Selection

In this section, we queried data from the WRDS database (CRSP-COMPUSTAT MERGED) to extract relevant financial as well as non-financial variables as X-variables. The next section wil outline the selected variables in detail and the reason for selecting these variables. 

## Single Variable

This contains variables that are directly obtainable from WRDS database. It will be organized based on the general account type it falls under, with reasons why it was selected.

### Assets 

+---------------+--------------------------+
| Variable Name | Accounting Name          |
+:=============:+:========================:+
| act           | Current Assets - Total   |
+---------------+--------------------------+

These variables provide an overall idea of the scale of the company. Because companies may engage in fraud that will result in their assets being overstated, assets are necessary to be included in the analysis. While the balances of more specific sub-classifications of assets may be more informative and indicative of any specific fraud risk, these variables are still chosen as a machine learning algorithm may have difficulties developing a consistent rule or generalisation based on more specific sub-classifications of assets. One such example is that a company may choose to manipulate Property, Plant and Equipment - Total (Net) account while another may choose to manipulate Receivables Total account.

Companies may generally be more incentivised to manipulate assets by recording assets they neither control nor own, or by inflating the fair value of their assets. Examples of fair value inflation include improper estimation of the amounts of future cash flows, improper estimation of the timing of future cash flows, and utilising an improper discount rate. Companies may also treat reductions in fair value as temporary impairments inappropriately. Alternatively, assets may be written down and when subsequently sold, allowing for greater profits/gains to be recorded.

#### Fixed Assets

+---------------+---------------------------------------------+
| Variable Name | Accounting Name                             |
+:=============:+:===========================================:+
| ppent         | Property, Plant and Equipment - Total (Net) |
+---------------+---------------------------------------------+

Fixed assets, such as property, plant and equipment, are long-term assets. If the revaluation model is used for these assets, fraud can be perpetuated by overstating the respective fair values. Companies may also retain obsolete and unused assets on their statement of financial position, which not only overstates assets but also overstates net income since losses incurred when disposing these assets are not accounted for.

#### Goodwill

+---------------+-----------------------+
| Variable Name | Accounting Name       |
+:=============:+:=====================:+
| gdwl          | Goodwill              |
+---------------+-----------------------+

As an intangible asset, goodwill may be inflated without any consequences because physically they may not have value. As such, there is a high possibiltiy to goodwill being overstated, or its amortization being understated without any valid reason.

### Liabilities

+---------------+--------------------------------+
| Variable Name | Accounting Name                |
+:=============:+:==============================:+
| dltt          | Long-term Debt                 |
+---------------+--------------------------------+

Liabilities represent a company's financial obligations to third parties. Companies have an incentive to understate their liabilities to make their statements of financial position appear healthier, as well as to avoid breaching debt covenants. Other ways of concealing liabilities include taking loans from related entities and writing off debts as forgiven bad debts.

Companies may also overstate liabilities if they are practising cookie-jar accounting, which involves creating reserves during years of good performance; the reserves are subsequently tapped into to inflate performance in bad years. Such liabilities may include things like provisions for restructuring or unearned income. Through this, companies can smoothen out volatility in their financial performance, misleading investors into believing that they are consistently meeting earnings targets.

### Income

#### Revenue

+---------------+-------------------------------+
| Variable Name | Accounting Name               |
+:=============:+:=============================:+
| revt          | Revenue - Total               |
+---------------+-------------------------------+

Companies have a strong incentive to overstate revenue to improve their apparent financial performance. Revenue can be overstated via several methods: fictitious sales from fictitious customers, fictitious sales from real customers, modification of invoices for actual sales, recording revenue in the wrong period, recording revenue as earned instead of unearned, variable selling prices, channel stuffing (sell retailers more products than they are expected to sell and round-tripping), which involves making a sale to another party, often a related entity, then buying back the same item. 

### Expenses

+---------------+----------------------+
| Variable Name | Accounting Name      |
+:=============:+:====================:+
| xlr           | Staff Expense - Total|
+---------------+----------------------+

Expenses may be understated in order to fraudulently inflate profit levels. One method would be capitalising expenses improperly. Alternatively, expenses may be deferred to later periods. Another measure, which may be used in poorly performing years, is known as a big-bath strategy and may involve, among other methods, recording write-offs, which are subsequently reversed to inflate profits.

#### Cost of Goods Sold

+---------------+--------------------+
| Variable Name | Accounting Name    |
+:=============:+:==================:+
| cogs          | Cost of Goods Sold |
+---------------+--------------------+

The cost of goods sold may be understated if methods like inventory overstatement are employed. 

#### Amortization and Depreciation

+---------------+----------------------+
| Variable Name | Accounting Name      |
+:=============:+:====================:+
| dp            | Depreciation - Total |
+---------------+----------------------+

Amortization and depreciation may be manipulated by adjusting the useful lives of assets, resulting in a reduction in expense. 

### Profit

+---------------+-------------------------------------+
| Variable Name | Accounting Name                     |
+:=============:+:===================================:+
| ebit          | Earnings Before Interest and Taxes  |
+---------------+-------------------------------------+

Companies have an incentive to overstate profit, and this can be done by overstating revenue and/or understating expenses. An important implication of earning manipulation is that it is a combination of aggressive interpretation and application of accounting rules. This signals that if profit manipulation is present, other variables may be manipulated to the benefit of the firm as well.
 
### Cash Flow

+---------------+-----------------------------------------+
| Variable Name | Accounting Name                         |
+:=============:+:=======================================:+
| dv            | Cash Dividends (Cash Flow)              |
+---------------+-----------------------------------------+

Cash flows may be inflated to improve a company's reported performance. Companies may manipulate cash flow via several methods. One is manipulating accounts payable, possibly by writing cheques and not deducting the amount on the cheques. Another would be to speed up the recognition of receivables, though this leads to poorer cash flows in the subsequent period. Companies may also classify non-operating cash flows as operating cash flows.  

### Shares

+---------------+--------------------------------------------------------------------+
| Variable Name | Accounting Name                                                    |
+:=============:+:==================================================================:+
| bkvlps        | Book Value Per Share                                               |
+---------------+--------------------------------------------------------------------+

Shares represent how profitable a company is. As a company becomes more profitable, then shares price as well as the payout will usually be high as well. Of course, companies will try to offer high shares earnings so as to prove that they are profitable and worth investing in.

## Ratios

Ratios and indexes are indicators of a company's health and may be manipulated to deceive users of financial statements such as creditors and investors. Alternatively, unusual values for certain ratios and indexes may be a sign that some underlying manipulation is taking place.

+----------------------------+--------------------+---------------------+
| Ratio Name                 |     Ratio Code     |       Formula       |
+:==========================:+:==================:+:===================:+
| Leverage Index             |   leverage_index   |     $$dltt/act$$    |
+----------------------------+--------------------+---------------------+
| Depreciation Index         |    depre_index     |  $$dp/(dp+ppent)$$  |
+----------------------------+--------------------+---------------------+
| Staff Administration Index |   saleadmin_index  |     $$xlr/revt$$    |
+----------------------------+--------------------+---------------------+

Significant decrease in Leverage Index can be connected with understatement of liabilities and expenses. On the other hand, a high level of Leverage Index also indicates the high number of creditors, increasing the incentives for fraud. As for Deprecian Index, a decrease in depreciation may give rise to the possibility of company's attempts to fraudulently revise the useful life of depreciated assets or the depreciation method. This may result in exaggerated profits in the income statement of the company. Similar to the Leverage Index, Staff Admin Index may also indicate an understatement of liabilities, sales general expenses and administrative costs. Negative changes in this index may also be caused by revenue falsification.

# Exploratory Data Analysis

Before we proceed to the model training, we have to see the general distribution of our y-variable (Restate_Int) with respect to certain key criteria such as Company Code, certain time series, as well as X-Values.

## Distribution of Misstatements

```{r include=FALSE}
library(dplyr)
df = read.csv("Restate_int_train.csv", stringsAsFactors = F)
#check the structure
str(df)
#change gvkey, year, restate to a factor
df$gvkey = as.factor(df$gvkey)
df$year = as.factor(df$year)
df$Restate_Int = as.factor(df$Restate_Int)
percent_intent = summary((df$Restate_Int))[2] / (summary((df$Restate_Int))[1] + 
summary((df$Restate_Int))[2]) * 100 
```

```{r echo=FALSE}
summary(df)
```

Generally, looking at the summary of the data, we can conclude that there is a low occurence of intentional accounting errors. There are in total **`r percent_intent`%** of datapoints that classifies as an intentional error (Restate_Int = 1).  

```{r, include=F}
library(ggplot2)
library(scales)
library(gridExtra)
```

```{r, fig.height = 10, fig.width = 12, echo=F}
#creation of important variables
#month
df = mutate(df, month = ifelse(substr(df$Date,2,2) == "/", substr(df$Date,1,1), substr(df$Date,1,2)))
df$month = as.integer(df$month)
df$month = as.factor(df$month)
#quarter
df = mutate(df, quarter = 
        ifelse(month == 1 | month == 2 | month == 3, 1,
        ifelse(month == 4 | month == 5 | month == 6, 2,
        ifelse(month == 7 | month == 8 | month == 9, 3, 4))))
df$quarter = as.integer(df$quarter)
df$quarter = as.factor(df$quarter)
#by company
df2 = head(df, 250)
ggplot(data = df2, aes(x = df2$gvkey)) +
labs(title = "Distribution of Intentional Error by Company",
     x = "Company Code", 
     y = "Frequency") + 
geom_bar(aes(fill = df2$Restate_Int), width = 0.35) +
scale_fill_discrete(name = "Intentional Error?",
labels = c("Not Intentional","Intentional")) + 
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 20)) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

To better visualize the distribution, taking only top 250 data points, based on the plot above, there are lots of companies with no intentional error (red bars) at all. Using the full dataset, only **`r 100 - (313/4237*100)`%** of the data consist of misstatements, further proving that this occurrence is very rare. 

In addition to that, there is also a lot of companies with incomplete data (out of the 5 years, only have 1 year). This could be due to bankruptcy/delisting, recent IPO or just missing during data collection. 

## Time Series Distribution of Misstatements
```{r, fig.height = 10, fig.width=12, echo = F,}
#identify companies with intentional error
df1 = read.csv("Restate_int_train.csv", stringsAsFactors = F)
df1 = df1 %>%
  group_by(gvkey) %>%
  mutate(Total = sum(Restate_Int)) %>%
  subset(Total != 0) %>%
  ungroup()
df1$gvkey = as.factor(df1$gvkey)
df1$Restate_Int = as.factor(df1$Restate_Int)
#month
df1 = mutate(df1, month = ifelse(substr(df1$Date,2,2) == "/", substr(df1$Date,1,1), substr(df1$Date,1,2)))
df1$month = as.integer(df1$month)
df1$month = as.factor(df1$month)
#quarter
df1 = mutate(df1, quarter = 
        ifelse(month == 1 | month == 2 | month == 3, 1,
        ifelse(month == 4 | month == 5 | month == 6, 2,
        ifelse(month == 7 | month == 8 | month == 9, 3, 4))))
df1$quarter = as.integer(df1$quarter)
df1$quarter = as.factor(df1$quarter)
df1 = head(df1, 250)
p1 = ggplot(data = df1, aes(x = df1$gvkey)) +
labs(title = "Distribution of Intentional Error by Company (at least 1 error)",
     x = "Company Code", 
     y = "Frequency") + 
geom_bar(aes(fill = Restate_Int), width = 0.35) +
scale_fill_discrete(name = "Intentional Error?",
labels = c("Not Intentional","Intentional")) + 
theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 20)) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
p1
```
To better visualize the distribution of misstatements, we took away all firms with 0 misstatements, and took the first 250 data points. Based on the above plot, we can see that among those with misstatements, Most of them only have 1 misstatement. It further proves that the occurrence is rare, although there are extremes values with 5 misstatements in 5 years. But we can still say that generally, the occurence is fairly rare.

```{r, fig.height=20, fig.width=12, echo = F}
#by year
p2=ggplot(data = df1, aes(x = df1$year)) +
scale_y_continuous(labels = percent) +
labs(title = "Distribution of Intentional Error by Year",
     x = "Year", 
     y = "Frequency") + 
geom_bar(aes(fill = df1$Restate_Int), width = 0.35, position = "fill") +
scale_fill_discrete(name = "Intentional Error?",
labels = c("Not Intentional","Intentional")) + 
theme(plot.title = element_text(hjust = 0.5))
#see by month
p3 = ggplot(data = df1, aes(x = df1$month)) +
scale_y_continuous(labels = percent) +
labs(title = "Distribution of Intentional Error by Month",
     x = "Month", 
     y = "Frequency") + 
geom_bar(aes(fill = df1$Restate_Int), width = 0.35, position = "fill") +
scale_fill_discrete(name = "Intentional Error?",
labels = c("Not Intentional","Intentional")) + 
theme(plot.title = element_text(hjust = 0.5))
#see by quarter
p4 = ggplot(data = df1, aes(x = df1$quarter)) +
scale_y_continuous(labels = percent) +
labs(title = "Distribution of Intentional Error by Quarter",
     x = "Quarter", 
     y = "Frequency") + 
geom_bar(aes(fill = df1$Restate_Int), width = 0.35, position = "fill") +
scale_fill_discrete(name = "Intentional Error?",
labels = c("Not Intentional","Intentional")) + 
theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p2,p3,p4,nrow = 3)
```
Looking at the same dataset after filtering out those with no misstatements, we also look at the distribution of misstatements on different timeframes (Year, Month and Quarter). 

When plotted against year, there isn't any clear trend over the 5 years. Against month, there also isn't any clear trend, we can only say there is generally a spike on quarter start and end. And lastly, against quarter, there's no trend but we can see that most misstatements happen in quarter 2. We can therefore conclude that time series analysis might not add a lot of value in the analysis.

## Missing Data Analysis

```{r, include = F}
#load datasets
data = read.csv("wrds.csv")
train = df[,c(1,2,5)]
test = read.csv("Restate_int_test.csv")

#take only the ones in variables selection 
data = data[,c('GVKEY','fyear','act','bkvlps','cogs','dltt','dp','dv','ebit','gdwl','ppent','revt','xlr')]

#rename columns and make variable year for merging
colnames(data)[1] = "gvkey"
colnames(data)[2] = "year"
```

```{r, include = F}
library(dplyr)
#outer join to create a training file
dftrain = merge(x = train, y = data, by = c('gvkey','year'), all.x = TRUE)
dftrain = mutate(dftrain, leverage_index = dltt/act, depre_index = dp/(dp+ppent), saleadmin_index = xlr/revt)[,-2]
#left join to create test file
dftest = left_join(test, data, by = c("gvkey","year"))[,c(-3,-4)]
dftest = mutate(dftest, leverage_index = dltt/act, depre_index = dp/(dp+ppent), saleadmin_index = xlr/revt)[,-2]
```

### Missing X variables

```{r}
apply(dftrain,2,function(x){sum(is.na(x))/length(x)*100})
```
Based on the percentage of missing data, most of the variables have decent level of missing data of around 15%. We then proceed to impute these values with 0, under the assumption that the data was not submitted because it does not exist.

## Correlation Analysis

```{r}
dfcorr = dftrain[,c(-1,-2,-3)]
cor(dfcorr)
```
Looking at the correlation matrix, a lot of the variables are quite heavily correlated, showing the prevalence of multi-collinearity. We are going to combat this by tuning and cross-validating our models, as well as utilizing bagging and boosting methods to reduce the overfitting effect of the multi-collinearity.

# Modeling Methods and Algorithms

## Logistic Regression
```{r, fig.height=20, fig.width=12, echo = F}
fit <- glm(Restate_int ~ ., data = df.train, family = binomial)
logodds <- predict(fit, df.test, type = "response")
```
Using a logistic regression model, data of the 15 identified features mentioned above was fed into the model, with a resulting out-of-sample AUC of **0.62484**.

This model result was further processed (post-processing) to improve the results of out-of-sample.

## GLMNET

We also tried a more advanced version of logistics regression using the Glmnet package. Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. We tried 3 methods under this, Ridge, LASSO and Elastic Net, which differs in the way the penalties are assigned. 

Generally they follow the same initial code:
```{r}
nonzero = function(x){ifelse(x < 0, 0,x)}
dftrain = as.data.frame(apply(dftrain,2,nonzero))
dftest = as.data.frame(apply(dftest,2,nonzero))[,-1]
library(glmnet)
library(coefplot)
Lasso_eq = as.formula(paste("Restate_Int ~ act + bkvlps + cogs + dltt + dp + dv + ebit + gdwl + ppent + revt + xlr + leverage_index + depre_index + saleadmin_index", collapse = ""))
x <- model.matrix(Lasso_eq, data=dftrain)[,-1] # [,-1] to remove intercept
y <- model.frame(Lasso_eq, data=dftrain)[,"Restate_Int"]
newX <- model.matrix(~., data=dftest) [,-1] #to remove intercept
```
Afterwards, the code for Ridge is as follows:
```{r}
cvfit = cv.glmnet(x=x, y=y,family = "binomial", type.measure="auc", alpha = 0)
plot(cvfit)
pred <- predict(cvfit, newx = newX, type="response", s = cvfit$lambda.min)
```

The code for LASSO is as follows:
```{r}
cvfit = cv.glmnet(x=x, y=y,family = "binomial", type.measure="auc", alpha = 1)
plot(cvfit)
pred <- predict(cvfit, newx = newX, type="response", s = cvfit$lambda.min)
```

The code for Elastic Net is as follows:
```{r}
set.seed(90298) #for reproducibility
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <- cv.glmnet(x=x, y=y, family = "binomial", nfold = 10, type.measure = "auc", paralle = TRUE, alpha = i)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
cvfit = cv.glmnet(x=x, y=y,family = "binomial", type.measure="auc", alpha = cv3$alpha)
plot(cvfit)
pred <- predict(cvfit, newx = newX, type="response", s = cvfit$lambda.min)
```

In order to get the prediction values, we used the minimum lambda as our choice of lambda.

### Results of Glmnet

Comparing the 3 methods, we found the Out-of-sample AUC scores as follows:

+-------------+-------------------+
| Method      | Out-of-sample AUC |
:+===========:+:=================:+
| Ridge	      | 0.59481		  |
+-------------+-------------------+
| LASSO       | 0.60344		  |
+-------------+-------------------+
| Elastic Net | 0.60349           |
+-------------+-------------------+

Based on the above, we can conclude that out of the 3 glmnet methods, Elastic Net gives the best out-of-sample AUC score.

## LightGBM

Besides traditional statistical methods, our group would like to try state-of-the-art machine learning technique. Hence, our group would like to try LightGBM.

LightGBM is developed by Microsoft and recently open-sourced to the community in July 2017 [source]( https://github.com/Microsoft/LightGBM). The reason we use LightGBM instead of the popular [XGBoost]( https://xgboost.readthedocs.io/en/latest/) is that while both can achieve the similar accuracy using ensemble tree method with reproducable result, LightGBM is indeed much faster than its counter part 

LightGBM, although it is a tree-based learning algorithm, is quite different from others like XGBoost. LightGBM grows tree vertically grows tree leaf-wise while other similar algorithm grow tree level-wise. Leaf chosen to grow are based on the maximum delta loss function

Below are the shor ilustration on the differences between the leaf-wise and level-wise leaf growing strategy 

[Insert leaf-wise tree growing]

[Insert level-wise tree growing]

Since Python is better in memory management ( especially in growing multiple tree at the same time), we decided to write our code in python 

```{python}
#get number of feature and number of training rows
X_train = train
X_test = test

num_train, num_feature = X_train.shape

#Get parameters 
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'auc',
    'num_leaves': 20,
    'learning_rate': 0.001,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 20,
    'verbose': 1000,
}
opt_params['params']['num_leaves'] = int(opt_params['params']['num_leaves'])
opt_params['params']['max_depth'] = int(opt_params['params']['max_depth'])
# print(int(opt_params['params']['num_leaves']))
params = opt_params['params']
params['metric'] = 'auc'
params['boosting_type'] = 'gbdt'
#------------------------------------------------------------
from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)


oof_preds = np.zeros(X_train.shape[0])
sub_preds = np.zeros(X_test.shape[0])
feature_importance_df = pd.DataFrame()
feature_name = [col for col in X_train.columns]
print(feature_name)
print('Starting training with stratified cross validation...')
for n_fold, (trn_idx, val_idx) in enumerate(folds.split(X_train,y)):
    print('fold {} '.format(n_fold))
    trn_x, trn_y = X_train[feature_name].iloc[trn_idx], y.iloc[trn_idx]
    val_x, val_y = X_train[feature_name].iloc[val_idx], y.iloc[val_idx]
    #Convert pandas data set into lgb format so that the package can read it
    lgb_train = lgb.Dataset(trn_x, trn_y)
    lgb_eval = lgb.Dataset(val_x, val_y)

    #Setting the training parameter
    gbm = lgb.train(params,
                lgb_train,
                num_boost_round=50000,
                valid_sets=lgb_eval,
                early_stopping_rounds=5000,
                   verbose_eval=1000)
    

    oof_preds[val_idx] = gbm.predict(val_x, num_iteration=gbm.best_iteration) #get oof prediction
    #predict on test set, take average
    sub_preds += gbm.predict(X_test[feature_name], num_iteration=gbm.best_iteration) / folds.n_splits 
    
    #save the feature important 
    fold_importance_df = pd.DataFrame()
    fold_importance_df["feature"] = feature_name
    fold_importance_df["importance"] = np.log1p(gbm.feature_importance(
        importance_type='gain',
        iteration=gbm.best_iteration))
    fold_importance_df["fold"] = n_fold + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
```

### Hyperparameter tuning:
Since tree based boosting method is very prone to overfitting, we would like to perform some hyperperameter tuning. There are many popular method for hyperparameter tuning :

- Random search
- Grid Search
- Bayesian Optimization

For exploration purposes, we tried out Bayesian Optimization with the help of `bayes_opt` python library. From the official github page, it was written that 

This is a constrained global optimization package built upon bayesian inference and gaussian process, that attempts to find the maximum value of an unknown function in as few iterations as possible. This technique is particularly suited for optimization of high cost functions, situations where the balance between exploration and exploitation is important.

The way bayesian optimization works is that it construct a posterior distribution of function (gausian process) that best describe the function that we want to optimize. In this case we want to optimize the hyperparameter to get the highest ROC AUC score. As the number of observation grow, the algorithm become more certain which region that are worth exploring and which are not, as seen in the picture below.

The visual illustration is given below:
[Insert the picture below ]

Below is the python code for our tuning process 
```{python}
X = train
def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=10, random_seed=6, n_estimators=50000, learning_rate=0.001, output_process=False):
    # prepare data
    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)
    # parameters
    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):
        params = {'application':'binary',
                  'learning_rate':learning_rate,
                  'boosting_type': 'gbdt',
                  'metric':'auc'}
        #Manually add in parameter that is in the arg* of the functions 
        params["num_leaves"] = int(round(num_leaves))
        params['feature_fraction'] = max(min(feature_fraction, 1), 0)
        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)
        params['max_depth'] = int(round(max_depth))
        params['lambda_l1'] = max(lambda_l1, 0)
        params['lambda_l2'] = max(lambda_l2, 0)
        params['min_split_gain'] = min_split_gain
        params['min_child_weight'] = min_child_weight
        
        #perform cross validation
        cv_result = lgb.cv(params,
                           train_data,
                           nfold=n_folds,
                           seed=random_seed,
                           stratified=True,
                           num_boost_round= n_estimators,
                           early_stopping_rounds=5000,
                           metrics=['auc'])
        return max(cv_result['auc-mean'])
    # Calling the function to optimize
    # Indicate the range of the paramter that we want to explore 
    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (5,15 ),
                                            'feature_fraction': (0.1, 0.9),
                                            'bagging_fraction': (0.8, 1),
                                            'max_depth': (2, 7),
                                            'lambda_l1': (0, 5),
                                            'lambda_l2': (0, 3),
                                            'min_split_gain': (0.001, 0.1),
                                            'min_child_weight': (5, 50)}, random_state=0)
    # Initialize the optimization process 
    lgbBO.maximize(init_points=init_round, n_iter=opt_round)
    
    # output optimization process
    if output_process==True: lgbBO.points_to_csv("bayes_opt_result.csv")
    
    # return best parameters
    print(lgbBO.max)
    return lgbBO.max

print('Performing Bayesian Optimization')
opt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=20, n_folds=10,
									 random_seed=6, n_estimators=50000, learning_rate=0.05)
```
### Tuning Result
Below are the result of our report 

```{Python}
Performing Bayesian Optimization
|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_sp... | num_le... |
-------------------------------------------------------------------------------------------------------------------------
|  1        |  0.6126   |  0.9098   |  0.6722   |  3.014    |  1.635    |  4.118    |  34.07    |  0.04432  |  13.92    |
|  2        |  0.6153   |  0.9927   |  0.4068   |  3.959    |  1.587    |  4.84     |  46.65    |  0.008033 |  5.871    |
|  3        |  0.6126   |  0.804    |  0.7661   |  3.891    |  2.61     |  6.893    |  40.96    |  0.04669  |  12.81    |
|  4        |  0.6185   |  0.8237   |  0.6119   |  0.7168   |  2.834    |  4.609    |  23.66    |  0.02719  |  12.74    |
|  5        |  0.6135   |  0.8912   |  0.5547   |  0.09395  |  1.853    |  5.06     |  32.76    |  0.09443  |  11.82    |
|  6        |  0.6175   |  0.8869   |  0.2392   |  4.977    |  2.348    |  6.797    |  5.161    |  0.00634  |  5.264    |
|  7        |  0.6095   |  0.8598   |  0.8069   |  0.1164   |  0.3642   |  2.011    |  5.017    |  0.07182  |  5.468    |
|  8        |  0.614    |  0.9964   |  0.2003   |  4.742    |  2.993    |  2.241    |  27.28    |  0.03882  |  5.847    |
|  9        |  0.602    |  0.9805   |  0.1272   |  4.668    |  2.822    |  2.34     |  5.705    |  0.08523  |  14.27    |
|  10       |  0.6163   |  0.8      |  0.9      |  4.727    |  0.0      |  7.0      |  20.29    |  0.1      |  10.35    |
|  11       |  0.6122   |  0.8322   |  0.8906   |  0.1014   |  2.907    |  2.154    |  48.65    |  0.08147  |  5.433    |
|  12       |  0.6116   |  0.8283   |  0.7893   |  0.2574   |  2.968    |  6.993    |  7.793    |  0.0805   |  6.294    |
|  13       |  0.6155   |  0.8142   |  0.102    |  2.101    |  0.3094   |  4.015    |  49.96    |  0.003752 |  14.57    |
|  14       |  0.5976   |  0.8877   |  0.1254   |  0.1411   |  1.405    |  6.97     |  21.51    |  0.01773  |  14.81    |
|  15       |  0.6131   |  0.8009   |  0.7091   |  0.3051   |  0.05169  |  6.817    |  48.54    |  0.04701  |  5.28     |
|  16       |  0.6097   |  0.931    |  0.8767   |  3.642    |  2.903    |  2.273    |  10.64    |  0.008109 |  5.331    |
|  17       |  0.6129   |  0.9409   |  0.6768   |  4.536    |  2.998    |  2.207    |  49.93    |  0.05696  |  14.89    |
|  18       |  0.6139   |  0.8624   |  0.8285   |  4.434    |  0.05691  |  2.059    |  45.62    |  0.008032 |  9.003    |
|  19       |  0.616    |  0.8037   |  0.8645   |  4.88     |  2.56     |  6.806    |  49.79    |  0.08645  |  7.811    |
|  20       |  0.612    |  0.8411   |  0.7628   |  4.998    |  2.547    |  6.961    |  27.51    |  0.0716   |  5.039    |
|  21       |  0.6221   |  0.8009   |  0.8486   |  4.627    |  0.3377   |  6.087    |  5.139    |  0.01408  |  6.939    |
|  22       |  0.613    |  0.8958   |  0.8291   |  0.4021   |  0.07403  |  2.163    |  21.94    |  0.02942  |  7.211    |
|  23       |  0.6065   |  0.8114   |  0.1177   |  4.862    |  2.685    |  2.048    |  26.22    |  0.02326  |  13.65    |
|  24       |  0.616    |  0.9087   |  0.8885   |  0.07466  |  2.566    |  5.779    |  49.85    |  0.06142  |  13.62    |
|  25       |  0.6246   |  0.9454   |  0.8759   |  4.716    |  2.757    |  6.142    |  5.335    |  0.02947  |  10.77    |
=========================================================================================================================
```

From this,we can find out the best parameter is of such 
```
{'target': 0.6246018240462684, 'params': {'bagging_fraction': 0.9453921853224881, 'feature_fraction': 0.8758724811791585, 'lambda_l1': 4.715776837697188, 'lambda_l2': 2.7571325217441576, 'max_depth': 6.142352664871275, 'min_child_weight': 5.335296245530968, 'min_split_gain': 0.029474362387896753, 'num_leaves': 10.765327234377471}}
```

### Lightgbm result 

From the feature importance plot, we can see that most of the loss comes from `bkvlps'` . It shows that **(CAN SOMEONE HELP ME WITH THIS PART)**

The cross validation AUC score of the model is **0.64792** 

## Simple Neural Network

With the goal of `fast protyping, fast failing` mindset, our group also try a simple Neural Network. Our group use Keras API with Tensorflow backend over Pytorch. Although both framework are excellent for production grade  modelling, we decided to go with Keras because of its simple, easy to use API with less modification from the boiler plate code base

```
Layer (type)                 Output Shape              Param #   
=================================================================
dense_28 (Dense)             (None, 150)               2400      
_________________________________________________________________
batch_normalization_25 (Batc (None, 150)               600       
_________________________________________________________________
activation_25 (Activation)   (None, 150)               0         
_________________________________________________________________
dropout_25 (Dropout)         (None, 150)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 150)               22650     
_________________________________________________________________
batch_normalization_26 (Batc (None, 150)               600       
_________________________________________________________________
activation_26 (Activation)   (None, 150)               0         
_________________________________________________________________
dropout_26 (Dropout)         (None, 150)               0         
_________________________________________________________________
dense_30 (Dense)             (None, 150)               22650     
_________________________________________________________________
batch_normalization_27 (Batc (None, 150)               600       
_________________________________________________________________
activation_27 (Activation)   (None, 150)               0         
_________________________________________________________________
dropout_27 (Dropout)         (None, 150)               0         
_________________________________________________________________
dense_31 (Dense)             (None, 1)                 151       
=================================================================
```

The model architecture is very basic. We did a batch normalization after the first Dense group with a hope to stay away from the Vanishing Gradient pitfall. We also use a `drop out` rate of 0.2 with a hope to avoid overfitting too much since we are using 2000 `epochs` to train our model.

Our group feel that we have too little data to play with, hence, we use `epoch = 2000` with a hope that model can pick up more meaningful features . We also use large `batch_size` to make the training faster and also to minimize the problem of noisy data when using small `epoch`

Below are our code to run the model 
```{Python}
def auc(y_true, y_pred):
    auc = tf.metrics.auc(y_true, y_pred)[1]
    K.get_session().run(tf.local_variables_initializer())
    return auc


#Model NN definition
def create_model_nn(in_dim,layer_size=150):
    model = Sequential()
    model.add(Dense(layer_size,input_dim=in_dim))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.2))
    for i in range(2):
        model.add(Dense(layer_size))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
    model.compile(optimizer=adam,loss='binary_crossentropy',metrics = [auc])
    print(model.summary())
    return model
#_____________________________________________
sub_name = 'submission'
class_weights = None
weighted = True
balanced = False

#split the data into validation set and training set 
X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.1, random_state=1234,stratify = y)
#____________________________________________

#Class weights to handle the imbalanced dataset

if weighted:
    sub_name = sub_name+'_weighted'
    if balanced:
        class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(y_train),
                                                 y_train)
    else:
        class_weights = {
            1:50, 
            0:1
                }
print('creating model')
model_nn = create_model_nn(train.shape[1])
callback = EarlyStopping(monitor="val_auc", patience=500, verbose=0, mode='max')
history = model_nn.fit(X_train, y_train, validation_data = (X_test,y_test),epochs=2000,batch_size=4048,verbose=1,callbacks=[callback],class_weight=class_weights)
target_pred_nn = model_nn.predict(test)
print('\n Validation Max score : {}'.format(np.max(history.history['val_auc'])))
``` 
### Results
**<Please paste your result here>**
	
## Post-Processing

```{r, fig.height=20, fig.width=12, echo = F}
Restate_List = unique(subset(df.train, df.train$Restate_int == 1)[,1])
post_processing = function(x){
  ifelse(x %in% Restate_List,0.6,0)}

test1['boosting'] = lapply(test1['gvkey'],post_processing)
test1['Restate_Int'] = test1['Restate_Int'] + test1['boosting']
test1$Restate_Int = ifelse(test1$Restate_Int > 1,1,test1$Restate_Int)
test1 = test1[,c(1,2)]
```
One thing to take note from our training data is that we remove the `gvkey` and `fyear` column. Therefore, little information about the compare and time series factor is carried out into our model. 

Hence, we would like to perform some post-processing to incorporate more time-series related information into our prediction

From the business perspective, our group hypothesize that company that have major Accounting Error in the previous year are likely to have accounting error in the future. Specifically in this data, we observe that 2010 is right during recession period, hence company that previously have intentional accounting error (indication of weak company performance), is likely to have accounting mistatement again. Therefore, for those companies, we will boost the probability by 0.6 and cap at 1. The reason of choosing `0.6` is purely by trial and error but we also think that it should not influence too much on the probability

After running the post-processing code above, the AUC score improved for most of the models as outlined in the summary below: 

+-----------------------+------------------------+-----------------------+
| Method                | Before Post-Processing | After Post-Processing |
:+=====================:+:======================:+:=====================:+
| Logistics Regression  | 0.62484		 | 0.68785               |
+-----------------------+------------------------+-----------------------+
| Elastic Net           | 0.60349  	         | 0.67173	         |
+-----------------------+------------------------+-----------------------+
| RandomForest	 	| 			 | 	                 |
+-----------------------+------------------------+-----------------------+
| LightGBM              | 0.64792	         | 0.69951	         |
+-----------------------+------------------------+-----------------------+
| Simple Neural Network | 		         | 		         |
+-----------------------+------------------------+-----------------------+
	
## Discussion

There are a few factors that we want to discuss in choosing our final model: metric performance, complexity , model explainability and interpretability.

### Metrics

When talking about model performance, we always talk about relevant matric . In this case, the performance metric is set to be `ROC AUC`. With solely ROC AUC in mind, it would be very reasonable to use **Neural Network** with post processing for best peformance 

### Complexity 
But there are other factors that our group would like to consider. One of which is model complexity. In this case, with small set of data, it takes about 10 minutes to train with 2000 epochs. However, if the data set get bigger, it would be more computationally costly to train the Neural Nets model. 

Hence, Our group would like to suggest LightGBM for lower complexity 

## Model explainability 
As a data analyst/ data scientist, we often have to explain our model to non-technical people . Hence, a deep learning model or lightgbm model is no more than a black box to the upper management . Therefore, in this case, **Logistic Regression** seem like a more reasonable choice. LASSO would also be a good option with good regularization factor 

Overall, we would like to use LightGBM because its not computational cost is much lesser than Deep learning model while it is also easy to understand and explain in a more layman term to non-technical people . Such explanation could be found in this [page](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) by Tianqi Chen, the author of XGBoost

# References
http://www.theforensicauditor.com/2017/10/02/financial-statement-fraud/  
https://books.google.com.sg/books?id=S6Y2DwAAQBAJ&printsec=frontcover#v=onepage&q&f=false
https://xgboost.readthedocs.io/en/latest/tutorials/model.html 
